[{"id":0,"href":"/blogs/end-to-end-mlops-on-aws-part1-introduction/","title":"E2E MLOps on AWS: Part1 - Introduction","section":"Blogs","content":"\rEnd-to-End MLOps on AWS: Part1 - Introduction\r#\rAuthors: Palash Nimodia\r, Yogendra Yatnalkar\rLast Edited: 01/05/2023 The upcoming series of blogs will try to explain how to build a complete MLOps system on AWS, mainly revolving around the AWS SageMaker ecosystem. The Part-1 of this blog introduces the concept of MLOps, its various constituents and the AWS services that are needed to build them.\nPrerequisite: The blog series expects the reader to have a basic understanding of the following concepts:\nDocker Basic AWS Ecosystem (S3, ECR, AWS Systems Manager Parameter Store, etc) Advance knowledge of AWS SageMaker (and experience with Bring Your Own Container - BYOC for SageMaker jobs and SageMaker Pipelines) Basics of machine learning and deep learning techniques Why MLOps ?\r#\rAccording to the several sources available on the internet, around 85-90% of the Machine Learning models never make it into production. Some of the reasons for this low success rate are:\nDifficulty in scaling the tools and services around the AI ecosystem; and proving positive return on investment Difficulty in detecting drift and deciding what will be the best strategy to retrain the hosted models Complexity of governance and deployment of numerous AI models Lack of leadership support and understanding of machine learning models Most of the above listed concerns can be resolved using right MLOps practices, hence MLOps has been gaining prominence in recent years.\nMLOps Definition:\r#\rMLOps refers to a standard set of practices for businesses that want to deploy and manage ML models in production.\nAny MLOps system includes a number of tools for different purposes, e.g.\nCode Version Control: Git (GitHub, GitLab, Bitbucket, etc) Data Version Control: DVC, Git LFS, etc Testing and building services: Jenkins, AWS CodePipeline, etc Infrastructure as a code: Terraform, AWS CloudFormation Machine Learning Platform: AWS SageMaker, GCP VertexAI Workflow Orchestration: SageMaker Pipelines, Apache Airflow, AWS Step Functions, Kubeflow In this series of blogs, we will use SageMaker as our main toolbox. We will demonstrate how to manage the entire lifecycle of an ML model with automation for use cases in multiple domains.\nNOTE: For this blog, we will only assume that we are working with batch-workload i.e the use-case will carry out batch-inference and no real-time inference. In future blogs, we will also touch upon the topic of real-time workloads.\nDefinitions:\r#\rThe two essential constituents for any MLOps System are components and pipelines.\nComponents:\r#\rA Component can be defined as an independent ML functionality or an ML operation which is a part of the larger process. Few of the examples of components could be: data-processing, ML model training, model inference, etc.\nPipeline\r#\rA pipeline is a workflow which constitutes one or more components that execute a holistic task. Examples of pipelines include: training pipeline, inference pipeline, etc.\nPipeline and component representation With our assumption of batch-workload, let\u0026rsquo;s define 2 pipelines, namely:\nTraining Pipeline:\r#\rThe training pipeline will have a sequence of steps where the first step will process the training and validation datasets. The next step will carry out model training and tuning on the processed training and validation sets. In the monitoring step, we will learn the distribution of the training, which will be used to detect data drift. The last step will be the explainability step, where we will generate explainable insights from our ML model on the training and validation sets. Inference Pipeline:\r#\rThe inference pipeline will be running in the batch-inference setting where the first step will consist of processing of production data as required by the ML model. On completion of the processing step, the processed data will be passed to the trained ML model for Scoring. The next step will include drift detection on the production data, using the data distribution artifacts from the training pipeline. If drift is detected, then retraining the ML model on a new set of data is one way to resolve it. Lastly, the explainability step will be executed to generate explainable insights from our ML model on the production data. To execute the above pipeline steps, we will have to develop the respective components.\nWe believe that most of the MLOps workloads consisting of multiple pipelines can be covered with 4 components which are:\nProcessing Component Algorithm Component Monitoring Component Explainability Component We recommend that every component has 2 modes, which are the Train mode and the Serve mode. This is to ensure that for every operation the training data undergoes, there is a corresponding operation during inference.\nAs the name suggests, when a component is executed in the training pipeline, it will be running in the “Train” mode. Similarly, when a component is getting executed in the inference pipeline, it will be set to run in the “Serve” mode.\nWhy SageMaker?\r#\rAmazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment.\nOver time SageMaker has incorporated features that have made it an end to end platform for MLOps and reduced the need to integrate with other tools and services. Example:\nEarlier Now AWS Batch SageMaker BatchTransform Kinesis Firehose Endpoint data capture AWS Lambda for ML hosting SageMaker serverless endpoint AWS StepFunction of Managed MWAA\rSageMaker pipelines Amazon SQS SageMaker Asynchronous endpoints Amazon EMR Pyspark Kernel in Studio Notebook Our AWS Stack ?\r#\rAs we said earlier, our MLOps system is built on top of AWS Cloud. The AWS services required are:\nAWS S3: Stores large volumes of data AWS SageMaker: The processing component, monitoring component and explainability component for batch-workloads discussed above are developed using SageMaker Processing Job. The training component is developed using SageMaker Training Job. The batch-inference component is developed using SageMaker BatchTransform Job. The Training and Batch-Inference pipelines are built using SageMaker Pipelines AWS Systems Manager Parameter Store: Used as a key-value store and for version tracking Other misc. Services: AWS CloudWatch, AWS Lambda, AWS EventBridge How to create SageMaker Workloads ?\r#\rWe have seen how our components are developed on AWS using different types of SageMaker workloads, such as processing jobs, training jobs, batch transform jobs, and so on. To create these workloads, we have two main options: Use the pre-built containers that AWS provides and maintains. We only need to write our own code scripts that define the custom logic we want to run. This is called the “script” mode of execution. Build our own custom containers that include the compute logic we want to run. Push these containers to AWS ECR and give the ECR URI as the input to the SageMaker workload. This is called the “bring your own container (BYOC)” mode of execution. To handle complex dependencies and having full control over the modules, we recommend building custom containers and using the “BYOC” mode for SageMaker workload execution. For detailed information, please refer to the AWS Documentation: SageMaker Processing Job (BYOC): LINK\rSageMaker Training Job (BYOC): LINK1\rand LINK2\rSageMaker BatchTransform Job (BYOC): LINK\rFor an end-to-end example of SageMaker workloads with custom containers, please refer to the following example: LINK\rThe END\r#\rWe hope that you found this blog informative and useful. To summarize, we started our blog with the definition of MLOps and then discussed its core constituents which are components and pipelines. Later, we saw in detail what is the role of each component and how they can be developed. We ended our blog with discussion around BYOC, its importance and example links.\nPlease provide us your valuable feedback and stay tuned. The upcoming blogs will be released soon.\n"},{"id":1,"href":"/blogs/computer-vision/e2e-mlops-on-aws-p2.1-cv-simulation/","title":"E2E MLOps on AWS: Part2.1 - CV Simulation with Drift \u0026 Retraining","section":"Computer Vision","content":"\rEnd-to-End MLOPS on AWS: Part2.1 - Computer Vision Simulation with Drift \u0026amp; Retraining\r#\rAuthors: Palash Nimodia\r, Yogendra Yatnalkar\rLast Edited: 02/05/2023 Previous Blog Link: End-to-End MLOps on AWS: Part1 - Introduction\rIn our previous blog, we explored the concept of MLOps and its core elements which are components and pipelines. In this second part of our series, we will demonstrate how we created a comprehensive simulation for computer-vision batch workloads and later present the results of the same. Some of the key features of our simulation include drift monitoring, model retraining on drift and model redeployment.\nUse-case details:\r#\rThe goal is to create an MLOps system for Image Classification at a large scale. We will focus on batch-inference workloads only, which are tasks that involve processing large batches of images using machine learning models. The MLOps system is based on AWS cloud services, but we will discuss them in more detail in future blog posts. The goal of this second part of our blog series is to give you an overview of the MLOps system, show you how we conducted the simulation and present the simulation results.\nThe Image Classification MLOps system has four components and two pipelines. The four components are:\nProcessing Component: To batch process the images Algorithm Component: To train a CNN model and perform batch-inference using the same. Monitoring Component: To learn training image dataset distribution and detect drift on production images. Explainability Component: Explainable insight generation on the input images using the trained ML model. The two pipelines are:\nTraining Pipeline: The train pipeline encapsulates all the components listed above and runs them in the “Train” mode. Batch-Inference Pipeline: The batch-inference pipeline encapsulates all the components listed above and runs them in the “Serve” mode.\nNote: For more details on Train and Serve mode, please refer to the previous blog. The upcoming blogs will discuss regarding components and pipelines in detail (stay tuned)\nOverview of the entire MLOps system Simulation and its goal:\r#\rThe end-goal of the simulation is to test the robustness of the MLOps system discussed above. As a part of this simulation, we ran this system on 7 different datasets in a sequential manner. The 7 datasets include 1 original dataset and 6 variants of the same set with varying levels of drift.\nThe simulation was performed with complete automation and helped us answer the following questions:\nDoes the system detect drift when it should ? If it does, does it trigger the retraining pipeline and train on the new data? How does drift affect model performance and can retraining fix it? Dataset and its variations:\r#\rThe dataset used for the simulation was a sample of “Dog Breed Identification” dataset taken from Kaggle (original data link\r).\nThe sample dataset includes 6 classes out of 120 classes from the original dataset. The 6 classes are: boston_bull, chow, collie, dhole, doberman, golden_retriever.\nThe sample dataset was divided into train-set, validation-set and test-set where the strength of each class being:\nTrain-set: 408 image, Validation-set: 34 images, Test-set: 42 images The original sample set is further augmented with 6 varying levels of drift. Hence, in total there are 7 variations of the same set which are:\nDataset Augmentation Form Original Original Set, No augmentation Stage-1 Random flip Stage-2 Random rotate Stage-3 Random rotate + Random flip Stage-4 Random blur + Very small random rotate Stage-5 Random blur + Random rotate Noisy High-degree random blur + High-degree random rotate + Random flip The 7 variation sample dataset can be found on kaggle: Kaggle-Link\rThe augmentation code link: Github-Link\rDataset Visualization:\r#\rPlease Note: Since the display image is small, the random noise will not be easily visible to human eyes in the below plot\nSimulation Execution and Result:\r#\rWith all the components, pipelines and data variants, we are ready to start the simulation execution. Without getting into the specifics of the AWS implementation, let’s take a look at how we completed the whole simulation:\nWe maintained a key-value pair database (DB) which will store the output location of every pipeline run. We used AWS Parameter Store as our key-value store (DB).\nOn the first run, the training pipeline was run once on the original set and the DB was updated.\nNext, the Batch-Inference pipeline was hosted to run on a schedule. The schedule was set to 1 hour (example: 12:05, 13:05, ….etc). On AWS, this was done using AWS Event Bridge Scheduler.\nAt each scheduled trigger, the input to the inference pipeline was changed in a cyclical manner starting from the original test set. The input test-set cycle was as follows:\nOriginal Set → Stage-1 → Stage-2 → Stage-3 → Stage-4 → Stage-5 → Stage-Noisy → (Cycle complete) Original Hence, till the cycle completes, after every run, a slightly higher drift set will be passed as input to the inference pipeline. An important rule that we followed was: the training pipeline would use the same stage of training and validation set as the test set that caused drift. For example, if the stage-3 test set triggered the training pipeline, then the input will be set to the stage-3 training and validation. This means that the training pipeline would always run on the latest data available for each test set. Time TEST-SET TRAIN SET VALIDATION SET 11.05 Original Original Original 12.05 Stage-1 Stage-1 Stage1 13.05 Stage-2 Stage-2 Stage-2 14.05 Stage-3 Stage-3 Stage-3 15.05 Stage-4 Stage-4 Stage-4 For the input visualization, please refer to the above table. From there, we can clearly see that at every hour the test set path is changing and if drift gets detected at any point, the same hours training and validation set will be passed as input.\nAfter running the training pipeline once and setting up the inference pipeline to run on an hourly schedule as discussed above, we ran the simulation and its results are as follows:\nTime Input Test Path Drift Artifacts Trained on set KS Drift Detection result (Retrain) 12.05 Stage-1 Original FALSE 13.05 Stage-2 Original FALSE 14.05 Stage 3 Original TRUE 15.05 Stage 4 Stage 3 Train False 16.05 Stage 5 Stage 3 Train TRUE 17.05 Noisy Test Stage 5 Train FALSE 18.05 Original Stage 5 Train FALSE From the above results table, we can see that the “Time” column tells us at what time the batch-inference pipeline was triggered. The “Input Test Path” shows the test dataset inputted at each time interval.\nThe column “Drift Artifacts Trained On Set” shows us which training run was used to create the monitoring artifacts. We previously mentioned that we ran the Train pipeline once before the simulation using the Original train and validation set. Therefore, we can see that at 12.05, even though the inference pipeline takes stage-1 as the input test path, the monitoring artifacts are based on the original set.\nOne important observation is, at any point when the drift was detected i.e at 14.05 and 16.05, the training pipeline was re-run with the latest train and validation set (i.e stage-3 set at 14.05 and stage-5 set at 16.05). Hence, at 15.05, when the inference pipeline was running on stage-4 test-set, the monitoring artifacts were trained on stage-3 train-set.\nWe ran the pipeline for 10+ hours, recorded all the results and concluded the simulation process. We can say that, we were successful in building a system which actively detected drift and tackled it with proper retraining strategy.\nAfter completing the simulation, we were left with only 1 question, do we actually need retraining to tackle drift ?\nNeed for retraining ?\r#\rThe inference pipeline only received the test-set as input and not its ground-truth labels, hence we could not verify whether our model performance had really deteriorated on the subsequent drift sets or not.\nSince this was a simulation of production and not actual production, we had access to the ground-truth labels for all the test-sets. We conducted a small experiment, where we trained multiple CNN models on one of the sets and measured the test-set accuracy and loss across all variations of the drifted test-sets.\nThe simulation result shows that drift occurred on the stage-3 set when the ML model and monitoring artifacts were trained on the original set. Therefore, we need to compare the performance of our model trained on the original set and stage-3 set with all the other sets.\nML Model Trained on set Metric Original Test Set Stage-1 Test Set Stage-2 Test Set Stage-3 Test Set Stage-4 Test Set Stage-5 Test Set Noisy Test Set Original Accuracy 97 97 97 92.8 95 92.8 95 Loss 0.17 0.17 0.19 0.13 0.33 0.25 0.27 Sage-3 Accuracy 100 100 100 100 97 100 100 Loss 0.01 0.0001 0.004 0.001 0.1 0.005 0.02 The training results table above and the simulation result table from before show a clear correlation between drift detection and model accuracy degradation. When artifacts were trained on the original set and drift was detected on the stage-3 set, the model accuracy dropped by about 4% (from stage-2 test set to stage-3 test set).\nHowever, after retraining the model on the stage-3 train set and testing it on the stage-4 train set, the model accuracy did not fall below a certain threshold. This suggests that retraining is an effective way to deal with drift.\nThe END\r#\rConclusion: This is the first post in our Computer Vision domain specific series, where we briefly explained our Image Classification MLOps system. We also covered the various datasets we used, the simulation method and the simulation result. Finally, we found out that retraining is an effective way to deal with drift.\nPlease provide us your valuable feedback and stay tuned. The upcoming blogs will be released soon.\n"},{"id":2,"href":"/blogs/computer-vision/e2e-mlops-on-aws-p2.2-cv-components-and-pipelines/","title":"E2E MLOps on AWS: Part2.2 - CV Components and Pipelines Deep Dive","section":"Computer Vision","content":"\rEnd-to-End MLOPS on AWS: Part2.2 - Computer Vision Components and Pipelines Deep Dive\r#\rAuthors: Palash Nimodia\r, Yogendra Yatnalkar\rLast Edited: 20/06/2023 Previous Blog Link: E2E MLOps on AWS: Part2.1 - Computer Vision Simulation with Drift \u0026amp; Retraining\rIn part 2.1 of our series, we gave a high-level overview of an MLOps system that handles large-scale image classification, end-to-end simulation and retraining evaluation to address drift. In this part 3, we will dive deeper into the computer vision components and pipelines that make up this system. Specifically, we will cover: how we designed components for batch workloads, how we implemented them on AWS using SageMaker and how we orchestrated them into MLOps pipelines using SageMaker Pipelines.\nComponent Mode and SageMaker Service Quick Recap:\r#\rA component is an independent ML functionality or an ML operation which is a part of the larger process. A pipeline is a workflow which constitutes one or more components that execute a holistic task.\nThere are 4 components and 2 pipelines as seen from image above, where each component has 2 modes, which are:\nTrain mode and Serve mode. Each component mode will be executed with any one of the following Sagemaker Job which are:\nSageMaker Training Job or SageMaker Processing Job or SageMaker BatchTransform Job As discussed in part1 of our blog, all the sagemaker workloads are developed using docker containers (BYOC - Bring Your Own Container).\nComponents in detail:\r#\r1. Processing Component:\r#\rThe processing component handles batch processing of large amounts of image data, using various data augmentation techniques. The batch-processing is usually performed before model training and inference.\nProcessing Component Lifecycle On AWS, the processing component will be built using Amazon Sagemaker Processing Jobs (link)\r. On completion of the SageMaker Processing job, the output processed data in saved back in AWS S3.\nIn the train mode, the processing job will transform train and validation data and in the serve mode, it will transform the production images.\nIn our processing component, the augmentation pipeline performed image resizing for both modes, while random rotation was only applied to train mode.\n2. Algorithm Component:\r#\rThe algorithm component performs 2 major tasks, which are model training and model inference.\nAlgorithm Component Lifecycle Unlike processing component, the 2 modes of algorithm component will perform completely different tasks.\nThe train mode will perform the task of ML model training on the training-dataset and computing the evaluation metric on the validation-dataset. Based on the ML model used, the train mode will also support model retraining capabilities. On AWS, the train mode of the algorithm component will be implemented using Sagemaker Training Job\r. In our use-case, we had trained a CNN model with ResNet as the base pre-trained model (ReseNet 50/100/150).\nIn serve mode, the production data is fed into the trained ML model for prediction. This model is the same one that was trained in train mode. On AWS, the serve mode of the algorithm component will be implemented using Sagemaker BatchTransform Job\r.\nThe data input to the algorithm component is processed train, validation or production data that are the outputs of the processing component.\n3. Monitoring Component:\r#\rModel monitoring refers to the process of closely tracking the performance of machine learning models in production. It enables us to detect any technical or performance issues, but mainly it is used to detect drift. There are two main types of drift:\nData drift: Drift when characteristics of the underlying data on which the model has trained changes over time. Concept Drift: Concept drift occurs when the characteristics of the target variable and its relationship with the training variables itself changes overtime. The easiest way to tackle drift (mainly data-drift) is by model retraining on the newly updated data.\nMonitoring Component Lifecycle On AWS, both the train and serve mode of the monitoring component are developed using SageMaker Processing Job.\nIn the train mode, the processing job learns the training images data distribution and saves it in a file which is also known as drift detection artifacts. In the serve mode, using the drift artifacts from the train mode, drift is identified on the production data. There are various popular algorithms/methods to develop a drift detection solution which can be seen from the below image:\nWays to detect drift In our monitoring component, we had used the Kolmogorov Smirnov\rstatistical test to identify drift between two probability distributions. This was implemented using a popular python package known as: alibi-detect\rThe data input to the monitoring component is usually the raw data itself, as augmentations performed by the processing component might change the input data distribution.\n4. Explainability Component:\r#\rAs the name suggests, the explainability component is used to understand the model interpretability and cause of its inference decision behind every inferred data sample. In any production MLOps system, explainable AI is very important as it adds accountability and compliance to our production system.\nExplainability Component Lifecycle On AWS, both the train and serve mode of the explainability component are developed using SageMaker Processing Job.\nThe data input to the explainability component is processed train, validation or production data which are the outputs of the processing component.\nThe second input during both the modes of the processing job is the trained ML model itself, which is the output of the algorithm component train mode.\nIn our case of CV image classification, the explainability component computes the integrated gradients for each data sample, from the production data in serve mode and training data in train mode using the trained classification model.\nPipelines in detail:\r#\rFrom the image classification use-case (part-2 blog), we are already aware that we have 2 pipelines which are:\nTraining Pipeline Batch-Inference Pipeline The 2 pipelines seen in the below image are developed using AWS SageMaker Pipelines (LINK)\rAbout SageMaker Pipelines:\r#\rAn Amazon SageMaker Model Building Pipelines pipeline is a series of interconnected steps that are defined using the Pipelines SDK. This pipeline definition encodes a pipeline using a directed acyclic graph (DAG) that can be exported as a JSON definition. This DAG gives information on the requirements for and relationships between each step of defined pipeline. The structure of a pipeline\u0026rsquo;s DAG is determined by the data dependencies between steps.\nSageMaker Pipelines We have already seen that each component has 2 modes of executions, which are the \u0026ldquo;Train\u0026rdquo; mode and \u0026ldquo;Serve\u0026rdquo; mode. All the components chained together in the training pipeline are set to run in the \u0026ldquo;Train\u0026rdquo; mode. Similarly, all the components chained together in the batch-inference pipeline are set to run in the \u0026ldquo;Serve\u0026rdquo; mode.\nThe inputs and outputs of each pipeline execution are stored in AWS S3. The S3 paths are then updated in AWS Parameter Store. This allows components from different pipelines to access each other\u0026rsquo;s outputs. The Parameter Store keys have a fixed prefix for each component output.\nAs we learned in blog 2 (part 2), the batch-inference pipeline runs on a schedule. We used AWS EventBridge to set up this scheduled trigger for the SageMaker pipeline.\nOn the other hand, we saw that when drift was detected in the batch-inference pipeline, the training pipeline was triggered automatically to tackle drift. This automatic trigger is carried out using AWS Lambda Function.\nThe component execution and pipeline execution logs are pushed to AWS CloudWatch.\nThe containers used to execute individual components are stored in AWS ECR (Elastic Container Registry).\nThe END\r#\rThank you for reading this blog. We hope you gained some new and valuable insights from it. In this blog, we explained the different components of our project and how we developed them on AWS. We also showed you how we used SageMaker Pipelines to orchestrate the workflow and automate the deployment.\nPlease provide us your valuable feedback and stay tuned. The code for the components and pipelines will be released soon....\n"},{"id":3,"href":"/blogs/time-series/time-series-forecasting-mlops-overview-and-simulation/","title":"E2E MLOps on AWS: Part3.1- TS Forecasting: MLOps Overview and Simulation","section":"Time Series","content":"\rEnd-to-End MLOps on AWS: Part3.1- Time Series Forecasting: MLOps Overview and Simulation\r#\rAuthors: Palash Nimodia\r, Abhishek Tawar\r, Steffi Andrade\rLast Edited: 01/07/2023 Previous Blog Link: End-to-End MLOps on AWS: Part1 - Introduction\rThis blog is a part of a series of blogs based on MLOps workflows we built. In this blog, we will be talking about the time series solution developed using AWS sagemaker pipeline. For more details and a deeper understanding of MLOps, components used and the types of components; go through our previous blog\r. You can find the solution code repository here\r.\nUse Case:\r#\rThe problem statement of this use case is to build an end to end MLOps solution for time series data. The solution will consist of a training pipeline and an inference pipeline built using sagemaker pipelines. The training pipeline will train an LSTM based forecasting model using the training data and save the model in s3. The inference pipeline will generate forecasts using the trained model in batch mode. It will also detect drift in model, data or both and retrain the model if drift was detected.\nAbout Dataset:\r#\rWe used an open source city temperature dataset from kaggle. It contains the daily average temperature values for multiple cities with few other features. This pipeline is built to forecast temperature of one city at a time hence we filter out data for ‘Birmingham’ city for simulation. The granularity of the dataset was at a daily level. We will forecast the average temperature value as it is the target column. You can download the dataset from this site\r.\nForecasting Parameters and Metrics:\r#\rBased on the granularity of the dataset used, the forecasting frequency was set to daily level. The look back is 15 days and the look ahead period is 7 days. We generated forecast for one day after the look ahead period (referred here as inference day/forecast day.) Forecasting Period Window Forecasting Period Window\nThe pipeline will be triggered on a daily basis to forecast the average temperature (Target column). The evaluation metric considered to measure the forecasting accuracy is MAPE (Mean Average Percentage Error). This metric will be used to identify if model drift is detected. High Level Workflow:\r#\rEnd-to-end AWS Sagemaker Pipeline The forecasting solution consists of two main pipelines: Training pipeline and Inference pipeline. Both pipelines consist of four main components that are tied together in a specific sequence to build the pipeline, they are:\nProcessing Algorithm Explainability Monitoring Inference pipeline:\nAll components in the inference pipeline are run in the serve mode. The inference pipeline will fetch the trained ML model, inference data and pipeline parameters. Using these inputs it generates temperature forecast in batch mode. Pipeline will also check for drift if the cooling period has passed. Cooling period prevents retraining pipeline from being triggered on consecutive days and waits for lookaheads period of time for the forecast from the new model to be evaluated for drift. If drift is detected, it will trigger the training pipeline and complete the execution of the inference pipeline. Training pipeline:\nAll components in the training pipeline are run in the train mode. Training pipeline takes the latest actuals data (split in to training set and validation set) as input. The pipeline trains a model and update the parameter store with the latest model artifact paths generated. This ensures that in the next run of the inference pipeline it uses the latest model artifacts. Simulation:\r#\rA simulation exercise was planned to test the entire ML lifecycle. For this simulation, we ran the pipeline as they would in an actual production environment. Since we worked with daily level data, we ran the simulation for 14 inference dates, from 01-03-2020 to 14-03-2020. The monitoring components can measure both data drift and model drift. We chose the retraining trigger to be model drift which is measured by degradation of MAPE value.\nEach run of the inference pipeline is defined in the table below:\nSimulation Run Dates Setup:\r#\rThe input data files required to generate inference for each ‘inference date’ for both pipelines in the simulation were saved in s3. When a pipeline runs for a particular inference day it will fetch the inference or train data file for that day. Every file was identified by including the corresponding inference date in its nomenclature.\nTwo lambda functions were created and scheduled. The first lambda function (named \u0026ldquo;Update-inference-date-lambda\u0026rdquo;) was created to update the inference date (increment by one day) parameter in the parameter store. This parameter defined the date for which the inference pipeline had to generate inference. The second lambda function (named \u0026ldquo;BT-pipeline-trigger-lambda\u0026rdquo;) was used to trigger the inference pipeline. Both lambda functions were scheduled using event bridge rules. The event rules were enabled to trigger the first lambda function 2 minutes before the second since this will make sure every new run of inference pipeline is different (next day of the previous run). Also note, eventbridge in scheduled mode cannot pass parameters to pipeline hence lambda (with event bridge rule) is a better trigger for pipeline.\nExecution:\r#\rTo run the simulation inference pipeline was scheduled to run every hour, for a period of 14 hours: to generate inference for 14 consecutive days. The table above shows the details of all the 14 runs. Before starting the inference pipeline runs we ran the train pipeline to train the model on latest actuals data (till date 22-02-2020). Inference pipeline also check if the cooling period is going on for the trained model which in our case is 7 days (= lookahead period). Hence, we will not check for model drift even in first 7 runs, and so the retraining pipeline will definitely won’t be triggered till 07-03-2020 inference date. For the first 10 runs no drift was detected. The data had natural drift which was detected on 11th run (highlighted with green in the table). Therefore, inference pipeline triggers the training pipeline. The training pipeline then trained a new model (with latest actuals data till 03-03-2020) and save the artifacts in s3, update the parameter store with new artifacts paths generated during re-training. In the next 3 runs inference pipeline generateed forecast with the new model. Conclusions:\r#\rOn the inference pipeline run for 11-03-2020, model drift was detected and training pipeline was triggered. There was natural drift in the data to cause model drift and hence there was no need to add external noise to the data. For the 3 dates which used retrained models, we also generated the forecast for these dates using the older model. This was done to check the effect of model retraining on forecast accuracy. The experiment shows that accuracy improved for all the dates after retraining. This blog gave you an overview of the end-to-end workflow of the time series forecasting pipeline developed using AWS sagemaker pipelines. In the next blog, we will discuss the working of the end-to-end sagemaker pipeline, the various steps involved and the AWS services used. You can find the next blog here\r.\nReferences:\r#\rhttps://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html\rhttps://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html\rhttps://sagemaker-examples.readthedocs.io/en/latest/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html\rhttps://awstip.com/how-to-automatically-trigger-a-sagemaker-pipeline-using-eventbridge-3b71829a9e5\r"},{"id":4,"href":"/blogs/time-series/time-series-forecasting-detailed-working-of-pipelines/","title":"E2E MLOps on AWS: Part3.2- TS Forecasting: Detailed working of Pipelines","section":"Time Series","content":"\rEnd-to-End MLOps on AWS: Part3.2- Time Series Forecasting: Detailed working of Pipelines\r#\rAuthors: Palash Nimodia\r, Abhishek Tawar\r, Steffi Andrade\rLast Edited: 01/07/2023 Previous Blog Link: E2E MLOps on AWS: Part3.1- Time Series Forecasting: MLOps Overview and Simulation\rThis blog is a part of a series of blogs explaining MLOps workflows we built. In this blog, we will talk about the end-to-end time series forecasting pipelines developed using AWS sagemaker pipeline. We will explain the working of the pipeline in detail including he steps involved and the various AWS services used. For more details and an overview of MLOps, the components used and the types of components; go through this blog\r). Also, for an overview of the time series pipeline refer to previous blog\r. You can find the solution code repository here\r.\nThe pipelines consists of components, each component has a specific task in the pipeline based on their function. Each component needs to be executed using an appropriate step in the sagemaker pipeline. We used the following steps in our pipelines: Lambda step, Processing step, Training step, Model step, Transform step, Condition step depending on the function of the corresponding component.\nThe Parameter store is a JSON file saved in S3. Alternatively, AWS Systems Manager Parameter Store could also be used.\nCommunication between Steps in the Pipeline:\r#\rSageMaker Pipeline steps define the actions that the pipeline takes and the relationship between steps using properties. Each step requires certain parameters to be pass. Three types of parameters are used in the pipelines: These parameters can be categorized into three types:\nSagemaker Pipeline Parameters:\nVariables are introduced into your pipeline definition using pipeline parameters and can be referenced throughout the pipeline. Parameters have a default value, which can be updated by specifying parameter values when starting a pipeline execution.\nParameter fetched from the parameter store:\nTo enable the model lifecycle management workflow it is required to version control important parameters. Some of such parameters are model artifacts location, drift baseline reports location, training data location etc. We maintain a parameter store to version all these parameters.\nParameter fetched from the previous step:\nThe step properties attribute is used to add data dependencies between steps in the pipeline. These properties are referenced as placeholder values and are resolved at runtime. The properties of one step are passed as input to another step in the pipeline.\nEnd-to-End Pipeline Flow:\r#\rAWS End-to-End Pipeline Architecture Diagram: Entries in the bracket indicate component names As discussed in the previous blogs, the end-to-end pipeline consists of two distinct sagemaker pipelines — Training pipeline and Inference pipeline. Both pipelines contain four distinct components i.e. processing, training, inference, explainability, and monitoring. Additionally, there are a few additional helper steps. Each component supports two run modes — Train mode and Serve mode. For the inference and training pipeline, every component will be running in the “Serve” mode and “Train” mode respectively. Let’s discuss the individual pipeline flows in detail —\nTraining Pipeline Flow: Training pipeline can be triggered by the inference pipeline when there is drift detected and the cooling period is over. The cooling period starts on the day the latest model was trained and continues for days equal to the look ahead period. The first step is the processing component that reads the training data, performs data cleaning/preprocessing and finally saves the training and validation data in s3. The algorithm component is the next step and it uses the traning data to train an LSTM model and validation data to calculate the validation accuracy. The generated model artifacts are saved in S3. Next the explainability component computes shap values. It also generates and saves a clarify report. This step is triggered only after the algorithm component step is completed. The monitoring component uses a sagemaker processing job to generate drift baselines/artifacts. It generates two kinds of baselines to detect drift in data and model and saves the drift baseline reports in s3. This step is triggered only after the algorithm component step is completed and runs in parallel with the explainability step. The last step of the training pipeline ‘update parameters’ is the lambda step that updates the parameter store with the latest model and other artifact paths generated by the training pipeline. These parameters act as pipeline input parameters for the inference pipeline. Inference Pipeline Flow: The first step in the pipeline is ‘get parameter’, it fetches the parameters from the parameter store and assigns them to the step properties which can be used throughout the pipeline. This step is defined as a lambda step. This step also checks if there is a forecast (generated by previous inference pipeline run) available for the actual date (Actual date is the latest date for which actuals are available = inference date - [Look ahead + 1] days). The first lambda step(of inference pipeline) sets the condition variable as “true”, if the forecast data is available else “false”. Next, the processing component reads the inference data, performs cleaning/preprocessing steps and finally saves the preprocessed data in s3. The algorithm component consists of two sub-steps. The first sub-step is the ‘model step’ where a sagemaker model is created using the model artifact path obtained from the parameter store. This sagemaker model is then used in the next step that is the ‘transform step’ to perform batch inference. The explainability component uses sagemaker processing jobs to compute shap values for the generated forecast. It also generates and saves a clarify report. The next step is the condition step which is executed in parallel with the explainability step as they both have dependency on the transform step. This condition step will reference the condition variable set by first step ‘get parameter’ and check if it is equal to “true”. If the condition is satisfied, it will execute the monitoring component and the final drift check lambda step else end the pipeline execution. The monitoring component detects drift in batch mode. It generates and saves the data and model drift report in s3. This step is also defined as a processing step and it is triggered if the condition step was satisfied. The final step is ‘Check Drift’ defined as a lambda step. It is used to first check if the cooling period is over and then check the output of the monitoring component to identify if drift was detected in data or model. If the cooling period is over and drift is detected, this step will trigger the training pipeline and complete the execution. We now have an understanding of the working of the end-to-end sagemaker pipeline, the various steps involved and the AWS services used. In the next and also the final blog of the series based on forecasting, we will deep dive into the functionality and working of the individual pipeline components in detail. You can find the next blog here\r.\nReferences:\r#\rhttps://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html\rhttps://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html\rhttps://sagemaker-examples.readthedocs.io/en/latest/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html\r"},{"id":5,"href":"/blogs/time-series/time-series-forecasting-components-deep-dive/","title":"E2E MLOps on AWS: Part3.3- TS Forecasting: Components Deep Dive","section":"Time Series","content":"\rEnd-to-End MLOps on AWS: Part3.3- Time Series Forecasting: Components Deep Dive\r#\rAuthors: Palash Nimodia\r, Abhishek Tawar\r, Steffi Andrade\rLast Edited: 01/07/2023 Previous Blog Link: E2E MLOps on AWS: Part3.2- Time Series Forecasting: Detailed working of Pipelines This blog is the third and last of a series of blogs based on MLOps workflows we built. In this blog, we will talk about the various components used in the timeseries pipeline developed using AWS sagemaker pipeline. Also, for an overview of the time series pipeline workflow refer to our first blog in Time-Series Domain\r. To understand the working of the pipeline in depth, the various steps and the AWS services involved, refer to our previous blog\r.\nComponents of the Sagemaker Pipeline From the diagram above we can observe that the end-to-end pipeline solution consists of two major pipelines: Training pipeline and Inference pipeline. Both pipelines consists of four main components they are –\nComponents in Detail:\r#\rProcessing component Algorithm component Explainability component Monitoring component Every component supports two run modes namely the train mode and serve mode. All components are executed in the train mode for the training pipeline and in the serve mode for the inference pipeline. In each component we use parameter ‘run_mode’ to specify the run mode. You can find the solution code repository here\r. Let\u0026rsquo;s dive into every component to understand its working.\n1] Processing Component:\r#\rThe processing component handles data-preprocessing in batch mode. This component reads the input CSV file based on the inference date parameter, performs cleaning, preprocessing, scaling steps and saves the preprocessed data in s3. This component will also apply specific filters to extract data for ‘Birmingham’ city. A processing job is created to run this component. The key differences in the run modes are:\nProcessing Component Lifecycle Train Mode:\nIn train mode it splits the data in train and validation splits. It saves the artifacts of the scalar.\nServe Mode:\nIn serve mode, it separates the target column from the input columns as ‘inference_Y.csv’ and ‘inference_X.csv’ respectively. The artifacts (scalers) output path of this component in train mode is passed as an input and used to scale inference_Y dataset.\n2] Algorithm Component:\r#\rThe algorithm component performs two major tasks, which are model training and model inference in batch setting. The key differences in the run modes are:\nAlgorithm Component Lifecycle Train Mode:\nIn train mode this component will perform the task of training the LSTM based model on the training dataset and evaluate the model on the validation dataset. The generated model artifacts are saved in S3. The train and validation data is passed as input to this component. Additionally, total number of epochs, features, early stopping value and look back period are passed as hyperparameters. The training is carried out in batch mode by creating a sagemaker training job.\nServe Mode:\nIn serve mode, it uses the model trained in the train mode to perform batch inference on the inference data. This component consists of two sub-steps in the inference pipeline. The first step is the ‘model step’ where a sagemaker model is created using the model artifact path obtained from the train mode output of this component. This sagemaker model is used in the next step that is the ‘transform step’ where a sagemaker batch transform job is created to perform batch inference on the inference data. Look back and look ahead period are also passed as environment variables. The transform step is carried out after the model step is completed. Generated forecast is saved in s3.\n3] Explainability Component:\r#\rThe explainability component is tasked with providing interpretability to the model. In order to achieve this task this component computes shap values by comparing the predicted and actual values. It then generates and saves a clarify report. It uses a sagemaker processing job to execute this task. The clarify reports generated are saved in s3. It is defined as a processing step within the training pipeline and it is triggered only after the algorithm component is executed. The key differences in the run modes are:\nExplainability Component Lifecycle Train Mode:\nThe input in the train mode is the train data.\nServe Mode:\nIn serve mode, the inputs to this component are the test data which is the output of the processing component in serve mode and the train data which is the output of the processing component in train mode.\n4] Monitoring Component:\r#\rThe monitoring component detects performance issues by detecting drift in the performance of machine learning models or data . It uses a sagemaker processing job to monitor the performance of models. It is defined as a processing step within the training pipeline and it is triggered only after the algorithm component is executed in parallel with the explainability component. The key differences in the run modes are:\nMonitoring Component Lifecycle Train Mode:\nIn train mode, the monitoring component generates two baselines to detect drift in data and model. It uses the train data to generate the baseline report for data drift and validation data for model drift. It saves the data and model drift baseline reports in s3 as CSV files.\nServe Mode:\nIn the serve mode, this component detects data and model drift. It will compare the detected drift with the baseline drift computed by this component in the train mode. Model drift is detected by comparing the forecast value generated with the actual value corresponding to that date. Since actual values will always lag the forecasted value, the date for which drift is detected is the date that is look ahead days before the inference date (drift_detect_date = inference date - [look_ahead+1] days). The inputs to this component are the inference data, forecast generated and actual value corresponding to the drift_detect_date and the monitoring baseline artifact path which is the output of the monitoring component in train mode. It generates and saves the drift analysis report in s3.\nWe have shared the code for all pipelines and components here. If you find our work interesting you can replicate it for your usecase. Feel free to contact us if you have any feedback, suggestions or comments. Happy learning.\nReferences:\r#\rhttps://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html\rhttps://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html\rhttps://sagemaker-examples.readthedocs.io/en/latest/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html\r"},{"id":6,"href":"/blogs/computer-vision/","title":"Computer Vision","section":"Blogs","content":""},{"id":7,"href":"/blogs/time-series/","title":"Time Series","section":"Blogs","content":""},{"id":8,"href":"/menu/","title":"Index","section":"Introduction","content":""},{"id":9,"href":"/blogs/","title":"Blogs","section":"Introduction","content":"\rIndex:\r#\rCommon Blog End-to-End MLOps on AWS: Part1 - Introduction\rComputer Vision Blogs E2E MLOps on AWS: Part2.1 - Computer Vision Simulation with Drift \u0026amp; Retraining\rE2E MLOps on AWS: Part2.2 - Computer Vision Components and Pipelines Deep Dive\rTime Series Blogs E2E MLOps on AWS: Part3.1- TS Forecasting: MLOps Overview and Simulation\rE2E MLOps on AWS: Part3.2- TS Forecasting: Detailed working of Pipelines\rE2E MLOps on AWS: Part3.3- TS Forecasting: Components Deep Dive\rAuthors:\r#\rDomain Authors Computer Vision Palash Nimodia\r, Yogendra Yatnalkar\rTime Series Forcasting Palash Nimodia\r, Abhishek Tawar\r, Steffi Andrade\rCode:\r#\rCode will be released soon\u0026hellip;\n"}]
[{"id":0,"href":"/blogs/end-to-end-mlops-on-aws-part1-introduction/","title":"E2E MLOps on AWS: Part1 - Introduction","section":"Blogs","content":"\rEnd-to-End MLOps on AWS: Part1 - Introduction\r#\rAuthors: Palash Nimodia\r, Yogendra Yatnalkar\rLast Edited: 01/05/2023 The upcoming series of blogs will try to explain how to build a complete MLOps system on AWS, mainly revolving around the AWS SageMaker ecosystem. The Part-1 of this blog introduces the concept of MLOps, its various constituents and the AWS services that are needed to build them.\nPrerequisite: The blog series expects the reader to have a basic understanding of the following concepts:\nDocker Basic AWS Ecosystem (S3, ECR, AWS Systems Manager Parameter Store, etc) Advance knowledge of AWS SageMaker (and experience with Bring Your Own Container - BYOC for SageMaker jobs and SageMaker Pipelines) Basics of machine learning and deep learning techniques Why MLOps ?\r#\rAccording to the several sources available on the internet, around 85-90% of the Machine Learning models never make it into production. Some of the reasons for this low success rate are:\nDifficulty in scaling the tools and services around the AI ecosystem; and proving positive return on investment Difficulty in detecting drift and deciding what will be the best strategy to retrain the hosted models Complexity of governance and deployment of numerous AI models Lack of leadership support and understanding of machine learning models Most of the above listed concerns can be resolved using right MLOps practices, hence MLOps has been gaining prominence in recent years.\nMLOps Definition:\r#\rMLOps refers to a standard set of practices for businesses that want to deploy and manage ML models in production.\nAny MLOps system includes a number of tools for different purposes, e.g.\nCode Version Control: Git (GitHub, GitLab, Bitbucket, etc) Data Version Control: DVC, Git LFS, etc Testing and building services: Jenkins, AWS CodePipeline, etc Infrastructure as a code: Terraform, AWS CloudFormation Machine Learning Platform: AWS SageMaker, GCP VertexAI Workflow Orchestration: SageMaker Pipelines, Apache Airflow, AWS Step Functions, Kubeflow In this series of blogs, we will use SageMaker as our main toolbox. We will demonstrate how to manage the entire lifecycle of an ML model with automation for use cases in multiple domains.\nNOTE: For this blog, we will only assume that we are working with batch-workload i.e the use-case will carry out batch-inference and no real-time inference. In future blogs, we will also touch upon the topic of real-time workloads.\nDefinitions:\r#\rThe two essential constituents for any MLOps System are components and pipelines.\nComponents:\r#\rA Component can be defined as an independent ML functionality or an ML operation which is a part of the larger process. Few of the examples of components could be: data-processing, ML model training, model inference, etc.\nPipeline\r#\rA pipeline is a workflow which constitutes one or more components that execute a holistic task. Examples of pipelines include: training pipeline, inference pipeline, etc.\nPipeline and component representation With our assumption of batch-workload, let\u0026rsquo;s define 2 pipelines, namely:\nTraining Pipeline:\r#\rThe training pipeline will have a sequence of steps where the first step will process the training and validation datasets. The next step will carry out model training and tuning on the processed training and validation sets. In the monitoring step, we will learn the distribution of the training, which will be used to detect data drift. The last step will be the explainability step, where we will generate explainable insights from our ML model on the training and validation sets. Inference Pipeline:\r#\rThe inference pipeline will be running in the batch-inference setting where the first step will consist of processing of production data as required by the ML model. On completion of the processing step, the processed data will be passed to the trained ML model for Scoring. The next step will include drift detection on the production data, using the data distribution artifacts from the training pipeline. If drift is detected, then retraining the ML model on a new set of data is one way to resolve it. Lastly, the explainability step will be executed to generate explainable insights from our ML model on the production data. To execute the above pipeline steps, we will have to develop the respective components.\nWe believe that most of the MLOps workloads consisting of multiple pipelines can be covered with 4 components which are:\nProcessing Component Algorithm Component Monitoring Component Explainability Component We recommend that every component has 2 modes, which are the Train mode and the Serve mode. This is to ensure that for every operation the training data undergoes, there is a corresponding operation during inference.\nAs the name suggests, when a component is executed in the training pipeline, it will be running in the “Train” mode. Similarly, when a component is getting executed in the inference pipeline, it will be set to run in the “Serve” mode.\nWhy SageMaker?\r#\rAmazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment.\nOver time SageMaker has incorporated features that have made it an end to end platform for MLOps and reduced the need to integrate with other tools and services. Example:\nEarlier Now AWS Batch SageMaker BatchTransform Kinesis Firehose Endpoint data capture AWS Lambda for ML hosting SageMaker serverless endpoint AWS StepFunction of Managed MWAA\rSageMaker pipelines Amazon SQS SageMaker Asynchronous endpoints Amazon EMR Pyspark Kernel in Studio Notebook Our AWS Stack ?\r#\rAs we said earlier, our MLOps system is built on top of AWS Cloud. The AWS services required are:\nAWS S3: Stores large volumes of data AWS SageMaker: The processing component, monitoring component and explainability component for batch-workloads discussed above are developed using SageMaker Processing Job. The training component is developed using SageMaker Training Job. The batch-inference component is developed using SageMaker BatchTransform Job. The Training and Batch-Inference pipelines are built using SageMaker Pipelines AWS Systems Manager Parameter Store: Used as a key-value store and for version tracking Other misc. Services: AWS CloudWatch, AWS Lambda, AWS EventBridge How to create SageMaker Workloads ?\r#\rWe have seen how our components are developed on AWS using different types of SageMaker workloads, such as processing jobs, training jobs, batch transform jobs, and so on. To create these workloads, we have two main options: Use the pre-built containers that AWS provides and maintains. We only need to write our own code scripts that define the custom logic we want to run. This is called the “script” mode of execution. Build our own custom containers that include the compute logic we want to run. Push these containers to AWS ECR and give the ECR URI as the input to the SageMaker workload. This is called the “bring your own container (BYOC)” mode of execution. To handle complex dependencies and having full control over the modules, we recommend building custom containers and using the “BYOC” mode for SageMaker workload execution. For detailed information, please refer to the AWS Documentation: SageMaker Processing Job (BYOC): LINK\rSageMaker Training Job (BYOC): LINK1\rand LINK2\rSageMaker BatchTransform Job (BYOC): LINK\rFor an end-to-end example of SageMaker workloads with custom containers, please refer to the following example: LINK\rThe END\r#\rWe hope that you found this blog informative and useful. To summarize, we started our blog with the definition of MLOps and then discussed its core constituents which are components and pipelines. Later, we saw in detail what is the role of each component and how they can be developed. We ended our blog with discussion around BYOC, its importance and example links.\nPlease provide us your valuable feedback and stay tuned. The upcoming blogs will be released soon.\n"},{"id":1,"href":"/blogs/e2e-mlops-on-aws-p2-cv-simulation/","title":"E2E MLOps on AWS: Part2 - Computer Vision Simulation with Drift \u0026 Retraining","section":"Blogs","content":"\rEnd-to-End MLOPS on AWS: Part2 - Computer Vision Simulation with Drift \u0026amp; Retraining\r#\rAuthors: Palash Nimodia\r, Yogendra Yatnalkar\rLast Edited: 02/05/2023 Previous Blog Link: End-to-End MLOps on AWS: Part1 - Introduction\rIn our previous blog, we explored the concept of MLOps and its core elements which are components and pipelines. In this second part of our series, we will demonstrate how we created a comprehensive simulation for computer-vision batch workloads and later present the results of the same. Some of the key features of our simulation include drift monitoring, model retraining on drift and model redeployment.\nUse-case details:\r#\rThe goal is to create an MLOps system for Image Classification at a large scale. We will focus on batch-inference workloads only, which are tasks that involve processing large batches of images using machine learning models. The MLOps system is based on AWS cloud services, but we will discuss them in more detail in future blog posts. The goal of this second part of our blog series is to give you an overview of the MLOps system, show you how we conducted the simulation and present the simulation results.\nThe Image Classification MLOps system has four components and two pipelines. The four components are:\nProcessing Component: To batch process the images Algorithm Component: To train a CNN model and perform batch-inference using the same. Monitoring Component: To learn training image dataset distribution and detect drift on production images. Explainability Component: Explainable insight generation on the input images using the trained ML model. The two pipelines are:\nTraining Pipeline: The train pipeline encapsulates all the components listed above and runs them in the “Train” mode. Batch-Inference Pipeline: The batch-inference pipeline encapsulates all the components listed above and runs them in the “Serve” mode.\nNote: For more details on Train and Serve mode, please refer to the previous blog. The upcoming blogs will discuss regarding components and pipelines in detail (stay tuned)\nOverview of the entire MLOps system Simulation and its goal:\r#\rThe end-goal of the simulation is to test the robustness of the MLOps system discussed above. As a part of this simulation, we ran this system on 7 different datasets in a sequential manner. The 7 datasets include 1 original dataset and 6 variants of the same set with varying levels of drift.\nThe simulation was performed with complete automation and helped us answer the following questions:\nDoes the system detect drift when it should ? If it does, does it trigger the retraining pipeline and train on the new data? How does drift affect model performance and can retraining fix it? Dataset and its variations:\r#\rThe dataset used for the simulation was a sample of “Dog Breed Identification” dataset taken from Kaggle (original data link\r).\nThe sample dataset includes 6 classes out of 120 classes from the original dataset. The 6 classes are: boston_bull, chow, collie, dhole, doberman, golden_retriever.\nThe sample dataset was divided into train-set, validation-set and test-set where the strength of each class being:\nTrain-set: 408 image, Validation-set: 34 images, Test-set: 42 images The original sample set is further augmented with 6 varying levels of drift. Hence, in total there are 7 variations of the same set which are:\nDataset Augmentation Form Original Original Set, No augmentation Stage-1 Random flip Stage-2 Random rotate Stage-3 Random rotate + Random flip Stage-4 Random blur + Very small random rotate Stage-5 Random blur + Random rotate Noisy High-degree random blur + High-degree random rotate + Random flip The 7 variation sample dataset can be found on kaggle: Kaggle-Link\rThe augmentation code link: Github-Link\rDataset Visualization:\r#\rPlease Note: Since the display image is small, the random noise will not be easily visible to human eyes in the below plot\nSimulation Execution and Result:\r#\rWith all the components, pipelines and data variants, we are ready to start the simulation execution. Without getting into the specifics of the AWS implementation, let’s take a look at how we completed the whole simulation:\nWe maintained a key-value pair database (DB) which will store the output location of every pipeline run. We used AWS Parameter Store as our key-value store (DB).\nOn the first run, the training pipeline was run once on the original set and the DB was updated.\nNext, the Batch-Inference pipeline was hosted to run on a schedule. The schedule was set to 1 hour (example: 12:05, 13:05, ….etc). On AWS, this was done using AWS Event Bridge Scheduler.\nAt each scheduled trigger, the input to the inference pipeline was changed in a cyclical manner starting from the original test set. The input test-set cycle was as follows:\nOriginal Set → Stage-1 → Stage-2 → Stage-3 → Stage-4 → Stage-5 → Stage-Noisy → (Cycle complete) Original Hence, till the cycle completes, after every run, a slightly higher drift set will be passed as input to the inference pipeline. An important rule that we followed was: the training pipeline would use the same stage of training and validation set as the test set that caused drift. For example, if the stage-3 test set triggered the training pipeline, then the input will be set to the stage-3 training and validation. This means that the training pipeline would always run on the latest data available for each test set. Time TEST-SET TRAIN SET VALIDATION SET 11.05 Original Original Original 12.05 Stage-1 Stage-1 Stage1 13.05 Stage-2 Stage-2 Stage-2 14.05 Stage-3 Stage-3 Stage-3 15.05 Stage-4 Stage-4 Stage-4 For the input visualization, please refer to the above table. From there, we can clearly see that at every hour the test set path is changing and if drift gets detected at any point, the same hours training and validation set will be passed as input.\nAfter running the training pipeline once and setting up the inference pipeline to run on an hourly schedule as discussed above, we ran the simulation and its results are as follows:\nTime Input Test Path Drift Artifacts Trained on set KS Drift Detection result (Retrain) 12.05 Stage-1 Original FALSE 13.05 Stage-2 Original FALSE 14.05 Stage 3 Original TRUE 15.05 Stage 4 Stage 3 Train False 16.05 Stage 5 Stage 3 Train TRUE 17.05 Noisy Test Stage 5 Train FALSE 18.05 Original Stage 5 Train FALSE From the above results table, we can see that the “Time” column tells us at what time the batch-inference pipeline was triggered. The “Input Test Path” shows the test dataset inputted at each time interval.\nThe column “Drift Artifacts Trained On Set” shows us which training run was used to create the monitoring artifacts. We previously mentioned that we ran the Train pipeline once before the simulation using the Original train and validation set. Therefore, we can see that at 12.05, even though the inference pipeline takes stage-1 as the input test path, the monitoring artifacts are based on the original set.\nOne important observation is, at any point when the drift was detected i.e at 14.05 and 16.05, the training pipeline was re-run with the latest train and validation set (i.e stage-3 set at 14.05 and stage-5 set at 16.05). Hence, at 15.05, when the inference pipeline was running on stage-4 test-set, the monitoring artifacts were trained on stage-3 train-set.\nWe ran the pipeline for 10+ hours, recorded all the results and concluded the simulation process. We can say that, we were successful in building a system which actively detected drift and tackled it with proper retraining strategy.\nAfter completing the simulation, we were left with only 1 question, do we actually need retraining to tackle drift ?\nNeed for retraining ?\r#\rThe inference pipeline only received the test-set as input and not its ground-truth labels, hence we could not verify whether our model performance had really deteriorated on the subsequent drift sets or not.\nSince this was a simulation of production and not actual production, we had access to the ground-truth labels for all the test-sets. We conducted a small experiment, where we trained multiple CNN models on one of the sets and measured the test-set accuracy and loss across all variations of the drifted test-sets.\nThe simulation result shows that drift occurred on the stage-3 set when the ML model and monitoring artifacts were trained on the original set. Therefore, we need to compare the performance of our model trained on the original set and stage-3 set with all the other sets.\nML Model Trained on set Metric Original Test Set Stage-1 Test Set Stage-2 Test Set Stage-3 Test Set Stage-4 Test Set Stage-5 Test Set Noisy Test Set Original Accuracy 97 97 97 92.8 95 92.8 95 Loss 0.17 0.17 0.19 0.13 0.33 0.25 0.27 Sage-3 Accuracy 100 100 100 100 97 100 100 Loss 0.01 0.0001 0.004 0.001 0.1 0.005 0.02 The training results table above and the simulation result table from before show a clear correlation between drift detection and model accuracy degradation. When artifacts were trained on the original set and drift was detected on the stage-3 set, the model accuracy dropped by about 4% (from stage-2 test set to stage-3 test set).\nHowever, after retraining the model on the stage-3 train set and testing it on the stage-4 train set, the model accuracy did not fall below a certain threshold. This suggests that retraining is an effective way to deal with drift.\nThe END\r#\rConclusion: This is the first post in our Computer Vision domain specific series, where we briefly explained our Image Classification MLOps system. We also covered the various datasets we used, the simulation method and the simulation result. Finally, we found out that retraining is an effective way to deal with drift.\nPlease provide us your valuable feedback and stay tuned. The upcoming blogs will be released soon.\n"},{"id":2,"href":"/blogs/e2e-mlops-on-aws-p3-cv-components-and-pipelines/","title":"E2E MLOps on AWS: Part3 - Computer Vision Components and Pipelines Deep Dive","section":"Blogs","content":"\rEnd-to-End MLOPS on AWS: Part3 - Computer Vision Components and Pipelines Deep Dive\r#\rAuthors: Palash Nimodia\r, Yogendra Yatnalkar\rLast Edited: 20/06/2023 Previous Blog Link: E2E MLOps on AWS: Part2 - Computer Vision Simulation with Drift \u0026amp; Retraining\rIn our previous blog (part 2), we saw a high level overview of an MLOps system for large-scale image classification, end-to-end simulation and retraining evaluation to tackle drift. In this part 3 of our series, we will explore the computer vision components in detail: how we developed them for batch workloads and how we built them on AWS using SageMaker.\nComponent Mode and SageMaker Service Quick Recap:\r#\rA component is an independent ML functionality or an ML operation which is a part of the larger process. A pipeline is a workflow which constitutes one or more components that execute a holistic task.\nThere are 4 components as seen from image above, where each component has 2 modes, which are:\nTrain mode and Serve mode. Each component mode will be executed with any one of the following Sagemaker Job which are:\nSageMaker Training Job or SageMaker Processing Job or SageMaker BatchTransform Job As discussed in part1 of our blog, all the sagemaker workloads are developed using docker containers (BYOC - Bring Your Own Container).\nComponents in detail:\r#\r1. Processing Component:\r#\rThe processing component handles batch processing of large amounts of image data, using various data augmentation techniques. The batch-processing is usually performed before model training and inference.\nProcessing Component Lifecycle On AWS, the processing component will be built using Amazon Sagemaker Processing Jobs (link)\r. On completion of the SageMaker Processing job, the output processed data in saved back in AWS S3.\nIn the train mode, the processing job will transform train and validation data and in the serve mode, it will transform the production images.\n2. Algorithm Component:\r#\rThe algorithm component performs 2 major tasks, which are model training and model inference.\nAlgorithm Component Lifecycle Unlike processing component, the 2 modes of algorithm component will perform completely different tasks.\nThe train mode will perform the task of ML model training on the training-dataset and computing the evaluation metric on the validation-dataset. Based on the ML model used, the train mode will also support model retraining capabilities. On AWS, the train mode of the algorithm component will be implemented using Sagemaker Training Job\r.\nIn serve mode, the production data is fed into the trained ML model for prediction. This model is the same one that was trained in train mode. On AWS, the serve mode of the algorithm component will be implemented using Sagemaker BatchTransform Job\r.\nThe data input to the algorithm component is processed train, validation or production data that are the outputs of the processing component.\n3. Monitoring Component:\r#\rModel monitoring refers to the process of closely tracking the performance of machine learning models in production. It enables us to detect any technical or performance issues, but mainly it is used to detect drift. There are two main types of drift:\nData drift: Drift when characteristics of the underlying data on which the model has trained changes over time. Concept Drift: Concept drift occurs when the characteristics of the target variable and its relationship with the training variables itself changes overtime. Monitoring Component Lifecycle On AWS, both the train and serve mode of the monitoring component are developed using SageMaker Processing Job.\nIn the train mode, the processing job learns the training images data distribution and saves it in a file which is also known as drift detection artifacts. In the serve mode, using the drift artifacts from the train mode, drift is identified on the production data. The data input to the monitoring component is usually the raw data itself, as augmentations performed by the processing component might change the input data distribution.\n4. Explainability Component:\r#\rAs the name suggests, the explainability component is used to understand the model interpretability and cause of its inference decision behind every inferred data sample. In any production MLOps system, explainable AI is very important as it adds accountability and compliance to our production system.\nExplainability Component Lifecycle On AWS, both the train and serve mode of the explainability component are developed using SageMaker Processing Job.\nThe data input to the explainability component is processed train, validation or production data which are the outputs of the processing component.\nThe second input during both the modes of the processing job is the trained ML model itself, which is the output of the algorithm component train mode.\nIn our case of CV image classification, the explainability component computes the integrated gradients for each data sample, from the production data in serve mode and training data in train mode using the trained classification model.\nPipelines in detail:\r#\rFrom the image classification use-case (part-2 blog), we are already aware that we have 2 pipelines which are:\nTraining Pipeline Batch-Inference Pipeline The 2 pipelines seen in the below image are developed using AWS SageMaker Pipelines (LINK)\rAbout SageMaker Pipelines:\r#\rAn Amazon SageMaker Model Building Pipelines pipeline is a series of interconnected steps that are defined using the Pipelines SDK. This pipeline definition encodes a pipeline using a directed acyclic graph (DAG) that can be exported as a JSON definition. This DAG gives information on the requirements for and relationships between each step of defined pipeline. The structure of a pipeline\u0026rsquo;s DAG is determined by the data dependencies between steps.\nSageMaker Pipelines We have already seen that each component has 2 modes of executions, which are the \u0026ldquo;Train\u0026rdquo; mode and \u0026ldquo;Serve\u0026rdquo; mode. All the components chained together in the training pipeline are set to run in the \u0026ldquo;Train\u0026rdquo; mode. Similarly, all the components chained together in the batch-inference pipeline are set to run in the \u0026ldquo;Serve\u0026rdquo; mode.\nThe inputs and outputs of each pipeline execution are stored in AWS S3. The S3 paths are then updated in AWS Parameter Store. This allows components from different pipelines to access each other\u0026rsquo;s outputs. The Parameter Store keys have a fixed prefix for each component output.\nAs we learned in blog 2 (part 2), the batch-inference pipeline runs on a schedule. We used AWS EventBridge to set up this scheduled trigger for the SageMaker pipeline.\nOn the other hand, we saw that when drift was detected in the batch-inference pipeline, the training pipeline was triggered automatically to tackle drift. This automatic trigger is carried out using AWS Lambda Function.\nThe component execution and pipeline execution logs are pushed to AWS CloudWatch.\nThe containers used to execute individual components are stored in AWS ECR (Elastic Container Registry).\nThe END\r#\rThank you for reading this blog. We hope you gained some new and valuable insights from it. In this blog, we explained the different components of our project and how we developed them on AWS. We also showed you how we used SageMaker Pipelines to orchestrate the workflow and automate the deployment.\nPlease provide us your valuable feedback and stay tuned. The code for the components and pipelines will be released soon....\n"},{"id":3,"href":"/blogs/","title":"Blogs","section":"Introduction","content":""}]